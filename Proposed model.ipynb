{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dominant-appeal"},"outputs":[],"source":["#This experiment to evaluate the effect of spectral\n","#features on detection task for each disfluency\n","#we will use MFCC+ASR+TD+att 10 experiments\n","#----------------------------------------------\n","import tensorflow as tf\n","from tensorflow.keras.layers import Permute,Input,add,GlobalAveragePooling1D,concatenate,Concatenate,MaxPooling1D,Bidirectional,Flatten,MaxPooling2D,GlobalAveragePooling2D,Lambda,Dropout,BatchNormalization,Dense,TimeDistributed,LayerNormalization,Reshape,LSTM,Conv1D,Conv2D,Activation\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler,ModelCheckpoint\n","\n","from keras import backend as K\n","from keras.models import Model, load_model\n","from keras import optimizers\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.utils.vis_utils import plot_model\n","\n","from sklearn import model_selection\n","from sklearn import metrics\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import precision_score\n","import librosa\n","import matplotlib.pyplot as plt\n","import librosa.display\n","\n","import pandas as pd\n","import numpy as np\n","\n","#----------------------------------------------"],"id":"dominant-appeal"},{"cell_type":"code","source":["!pip install tensorflow_addons\n","import tensorflow_addons as tfa"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMIvt3yKOre1","executionInfo":{"status":"ok","timestamp":1669527557007,"user_tz":0,"elapsed":2568,"user":{"displayName":"Abdulkarim Albanna","userId":"10002314189592944663"}},"outputId":"109b706f-888a-4edf-c612-12a64be6aff5"},"id":"zMIvt3yKOre1","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.18.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lobqS_zLYIZO"},"outputs":[],"source":["test_path=\"/content/drive/MyDrive/classification_model/Thesis_experiments/extra_experiments/optimized_model/data/test_fb_new.csv\"\n","train_path=\"/content/drive/MyDrive/classification_model/Thesis_experiments/extra_experiments/optimized_model/data/train_fb_new.csv\"\n","\n","\n","test=pd.read_csv(test_path)\n","test = test.iloc[1: , :]\n","\n","train=pd.read_csv(train_path)\n","train = train.iloc[1: , :]"],"id":"lobqS_zLYIZO"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rNzmF278q3O2","executionInfo":{"status":"ok","timestamp":1669527567415,"user_tz":0,"elapsed":1916,"user":{"displayName":"Abdulkarim Albanna","userId":"10002314189592944663"}},"outputId":"d903e8c5-3f65-4f85-92a5-3f2167a71de8"},"id":"rNzmF278q3O2","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnS2mIL5Z6aP"},"outputs":[],"source":["test.head(50)"],"id":"RnS2mIL5Z6aP"},{"cell_type":"code","execution_count":null,"metadata":{"id":"kM40pl8-Yj-Z"},"outputs":[],"source":["# # Drop first column of dataframe\n","# test = test.iloc[: , 1:]\n","\n","# # Drop first column of dataframe\n","# train = train.iloc[: , 1:]"],"id":"kM40pl8-Yj-Z"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vaSA5s_ZL3K"},"outputs":[],"source":["test"],"id":"4vaSA5s_ZL3K"},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0yUJYCMYrR8"},"outputs":[],"source":["y_train=train.iloc[:,-5:]\n","# # y_train=train.iloc[:,-5:-4]\n","# y_train=train.iloc[:,-1:]\n","\n","X_train=train.drop(train.iloc[:,-8:],axis=1)\n","\n","y_test=test.iloc[:,-5:]\n","# # y_test=test.iloc[:,-5:-4]\n","# y_test=test.iloc[:,-1:]\n","\n","X_test=test.drop(test.iloc[:,-8:],axis=1)"],"id":"x0yUJYCMYrR8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bigger-premises"},"outputs":[],"source":["#This Blcok to determine the stuttering events in our experiment\n","#By Kareem\n","#----------------------------------------------\n","disfluencies=['Prolongation','Block','SoundRep','WordRep','Interjection','NoStutteredWords']\n","# disfluencies=['Prolongation','Block','SoundRep','WordRep','Interjection']\n","# disfluencies=['Stutter','Normal']\n","[disfluency for disfluency in disfluencies]\n","#----------------------------------------------"],"id":"bigger-premises"},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwuvQIUM3V37"},"outputs":[],"source":["df_train=pd.DataFrame(X_train)\n","df_test=pd.DataFrame(X_test)"],"id":"uwuvQIUM3V37"},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgXA81CxpVRY"},"outputs":[],"source":["df_test"],"id":"WgXA81CxpVRY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQUa5AIK3Pf2"},"outputs":[],"source":["\n","\n","X_train_asr=df_train.iloc[:,-4768:]\n","df_train.drop(df_train.iloc[:,-4768:],axis=1,inplace=True)\n","\n","X_test_asr=df_test.iloc[:,-4768:]\n","df_test.drop(df_test.iloc[:,-4768:],axis=1,inplace=True)\n","\n","\n"],"id":"WQUa5AIK3Pf2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ACcZFLhaqDcd"},"outputs":[],"source":["X_train_ff=df_train.iloc[:,-903:]\n","df_train.drop(df_train.iloc[:,-903:],axis=1,inplace=True)\n","\n","X_test_ff=df_test.iloc[:,-903:]\n","df_test.drop(df_test.iloc[:,-903:],axis=1,inplace=True)"],"id":"ACcZFLhaqDcd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnJWMg9r2SPT"},"outputs":[],"source":["X_train_ac=df_train.iloc[:,-300:]\n","df_train.drop(df_train.iloc[:,-300:],axis=1,inplace=True)\n","\n","X_test_ac=df_test.iloc[:,-300:]\n","df_test.drop(df_test.iloc[:,-300:],axis=1,inplace=True)"],"id":"cnJWMg9r2SPT"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vRwIgHAIaLg"},"outputs":[],"source":["X_train_zc=df_train.iloc[:,-301:]\n","df_train.drop(df_train.iloc[:,-301:],axis=1,inplace=True)\n","\n","X_test_zc=df_test.iloc[:,-301:]\n","df_test.drop(df_test.iloc[:,-301:],axis=1,inplace=True)"],"id":"1vRwIgHAIaLg"},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9_D8Cg0pZyR"},"outputs":[],"source":["X_train_mfcc_2d=df_train\n","X_test_mfcc_2d=df_test"],"id":"J9_D8Cg0pZyR"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZpIpsR2lpXHO"},"outputs":[],"source":["X_test_mfcc_2d"],"id":"ZpIpsR2lpXHO"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UWQIGcnpC4F"},"outputs":[],"source":["X_test_ff"],"id":"8UWQIGcnpC4F"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mbY38A2DmJVY"},"outputs":[],"source":["y_train=y_train\n","y_train = y_train.astype(int)\n","\n","y_test=y_test\n","y_test = y_test.astype(int)"],"id":"mbY38A2DmJVY"},{"cell_type":"code","source":["y_test"],"metadata":{"id":"vKvr7J-408ZF"},"id":"vKvr7J-408ZF","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYZ11KzwkIrX"},"outputs":[],"source":["y_test"],"id":"wYZ11KzwkIrX"},{"cell_type":"code","execution_count":null,"metadata":{"id":"received-disabled"},"outputs":[],"source":["import seaborn as sns\n","categories = list(y_test.columns.values)\n","sns.set(font_scale = 2)\n","plt.figure(figsize=(15,9))\n","ax= sns.barplot(categories, y_test.iloc[:,:].sum().values)\n","\n","plt.title(\"Disfluency in each category\", fontsize=24)\n","plt.ylabel('Number of Disfluencies', fontsize=18)\n","plt.xlabel('Disfluency Type ', fontsize=18)\n","#adding the text labels\n","rects = ax.patches\n","labels = y_test.iloc[:,0:].sum().values\n","for rect, label in zip(rects, labels):\n","    height = rect.get_height()\n","    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n","plt.show()"],"id":"received-disabled"},{"cell_type":"code","execution_count":null,"metadata":{"id":"sfpX0fsb2JzR"},"outputs":[],"source":["x_train_asr=np.array(X_train_asr).reshape(len(X_train_asr),149,32)\n","x_valid_asr=np.array(X_test_asr).reshape(len(X_test_asr),149,32)\n","\n","\n","\n","x_train_ff=np.array(X_train_ff).reshape(len(X_train_ff),3,301)\n","x_valid_ff=np.array(X_test_ff).reshape(len(X_test_ff),3,301)\n","\n","x_train_ac=np.array(X_train_ac).reshape(len(X_train_ac),1,300)\n","x_valid_ac=np.array(X_test_ac).reshape(len(X_test_ac),1,300)\n","\n","x_train_zc=np.array(X_train_zc).reshape(len(X_train_zc),1,301)\n","x_valid_zc=np.array(X_test_zc).reshape(len(X_test_zc),1,301)\n","\n","x_train_mfcc_2d=np.array(X_train_mfcc_2d).reshape(len(X_train_mfcc_2d),20,301)\n","x_valid_mfcc_2d=np.array(X_test_mfcc_2d).reshape(len(X_test_mfcc_2d),20,301)\n","\n","# Y_train=np.array(y_train)\n","# y_valid=np.array(y_test)\n","# Y=Y_train\n","# y_test=y_valid\n","\n","\n","\n","X_mfcc_2d=x_train_mfcc_2d\n","X_asr=x_train_asr\n","\n","X_ff=x_train_ff\n","X_ac=x_train_ac\n","X_zc=x_train_zc\n","\n","\n"],"id":"sfpX0fsb2JzR"},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJ0froYU2s6N"},"outputs":[],"source":["\n","spect = X_train_mfcc_2d.iloc[9]\n","\n","spect=np.array(spect).reshape(20,301)\n","\n","print(spect.shape)\n","plt.figure(figsize=(10, 4))\n","librosa.display.specshow(spect, y_axis='mel', x_axis='time')\n","plt.colorbar(format='%+2.0f dB')\n","plt.show()"],"id":"OJ0froYU2s6N"},{"cell_type":"code","execution_count":null,"metadata":{"id":"779vKmeF2yiW"},"outputs":[],"source":["model_paramerters={\n","    'no_of_frames':40,\n","    'no_of_classes':1,\n","    'frequency':1,\n","    'no_channels':1,\n","    'no_conv_layers':1,\n","    'conv':{\n","        'kernal_size':(3,3),\n","        'kernal_size_1d':3,\n","        'padding':'same',\n","        'activation':'relu'\n","\n","    },\n","    'lstm':\n","    {\n","\n","    }\n","\n","\n","}\n","\n","def custom_f1(y_true, y_pred):\n","    def recall_m(y_true, y_pred):\n","        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","\n","        recall = TP / (Positives+K.epsilon())\n","        return recall\n","\n","\n","    def precision_m(y_true, y_pred):\n","        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","\n","        precision = TP / (Pred_Positives+K.epsilon())\n","        return precision\n","\n","    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n","\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n",""],"id":"779vKmeF2yiW"},{"cell_type":"code","source":["#https://github.com/kobiso/CBAM-keras/blob/master/models/attention_module.py\n","class ChannelAttention(tf.keras.layers.Layer):\n","      def __init__(self, filters, ratio):\n","        super(ChannelAttention, self).__init__()\n","        self.filters = filters\n","        self.ratio = ratio\n","\n","        def build(self, input_shape):\n","            self.shared_layer_one = tf.keras.layers.Dense(self.filters//self.ratio,\n","                             activation='relu', kernel_initializer='he_normal',\n","                              use_bias=True,\n","                              bias_initializer='zeros')\n","            self.shared_layer_two = tf.keras.layers.Dense(self.filters,\n","                             kernel_initializer='he_normal',\n","                             use_bias=True,\n","                             bias_initializer='zeros')\n","\n","        def call(self, inputs):\n","            # AvgPool\n","            avg_pool = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n","\n","\n","            avg_pool = self.shared_layer_one(avg_pool)\n","            avg_pool = self.shared_layer_two(avg_pool)\n","\n","            # MaxPool\n","            max_pool = tf.keras.layers.GlobalMaxPooling1D()(inputs)\n","            max_pool = tf.keras.layers.Reshape((1,1,filters))(max_pool)\n","\n","            max_pool = shared_layer_one(max_pool)\n","            max_pool = shared_layer_two(max_pool)\n","\n","\n","            attention = tf.keras.layers.Add()([avg_pool,max_pool])\n","            attention = tf.keras.layers.Activation('sigmoid')(attention)\n","\n","            return tf.keras.layers.Multiply()([inputs, attention])\n","\n","\n","class SpatialAttention(tf.keras.layers.Layer):\n","      def __init__(self, kernel_size):\n","        super(SpatialAttention, self).__init__()\n","        self.kernel_size = kernel_size\n","\n","        def build(self, input_shape):\n","            self.conv2d = tf.keras.layers.Conv2D(filters = 1,\n","                    kernel_size=self.kernel_size,\n","                    strides=1,\n","                    padding='same',\n","                    activation='sigmoid',\n","                    kernel_initializer='he_normal',\n","                    use_bias=False)\n","\n","        def call(self, inputs):\n","\n","            # AvgPool\n","            avg_pool = tf.keras.layers.Lambda(lambda x: tf.keras.backend.mean(x, axis=3, keepdims=True))(inputs)\n","\n","            # MaxPool\n","            max_pool = tf.keras.layers.Lambda(lambda x: tf.keras.backend.max(x, axis=3, keepdims=True))(inputs)\n","\n","            attention = tf.keras.layers.Concatenate(axis=3)([avg_pool, max_pool])\n","\n","            attention = self.conv2d(attention)\n","\n","\n","            return tf.keras.layers.multiply([inputs, attention])"],"metadata":{"id":"72gbuhZuqH6B"},"id":"72gbuhZuqH6B","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtZ2cVzJ32m1"},"outputs":[],"source":["def cnn_rnn_1d(x,no_filters,k_size,activation_function):\n","  x=tf.keras.layers.Conv1D(no_filters, kernel_size =3 ,strides=1, padding='same',activation=activation_function\n","                             )(x)\n","\n","  x=tf.keras.layers.BatchNormalization()(x, training=True)\n","  out=tf.keras.layers.Dropout(0.1)(x)\n","  return out\n","\n","def cnn_rnn_2d(x,no_filters,k_size,activation_function):\n","  x=tf.keras.layers.Conv2D(no_filters, kernel_size =3 ,strides=1, padding='same',activation=activation_function\n","                            )(x)\n","\n","  x=tf.keras.layers.BatchNormalization()(x, training=True)\n","  out=tf.keras.layers.Dropout(0.1)(x)\n","  return out"],"id":"mtZ2cVzJ32m1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"YcpncgQC38k9"},"outputs":[],"source":["\n","\n","\n","av=[]\n","def proposed_model_build():\n","  print('Generate disfluency model...')\n","\n","  input_asr=tf.keras.layers.Input(shape=(149,32))\n","\n","\n","\n","  input_ff=tf.keras.layers.Input(shape=(3,301))\n","  input_ac=tf.keras.layers.Input(shape=(1,300))\n","  input_zc=tf.keras.layers.Input(shape=(1,301))\n","  input_mfcc_2d=tf.keras.layers.Input(shape=(20,301,1))\n","\n","\n","\n","  x_asr=cnn_rnn_1d(input_asr,256,(3,3),'relu')\n","  x_asr=ChannelAttention(256, 8)( x_asr)\n","  x_asr=SpatialAttention(7)(x_asr)\n","\n","\n","  x_ff=cnn_rnn_1d(input_ff,256,(3,3),'relu')\n","  x_ff=ChannelAttention(256, 8)( x_ff)\n","  x_ff=SpatialAttention(7)(x_ff)\n","\n","  x_ac=cnn_rnn_1d(input_ac,256,(3,3),'relu')\n","  x_ac=ChannelAttention(256, 8)( x_ac)\n","  x_ac=SpatialAttention(7)(x_ac)\n","\n","  x_zc=cnn_rnn_1d(input_zc,256,(3,3),'relu')\n","  x_zc=ChannelAttention(256, 8)( x_zc)\n","  x_zc=SpatialAttention(7)(x_zc)\n","\n","  x_mfcc_2d=cnn_rnn_2d(input_mfcc_2d,256,(3,3),'relu')\n","  x_mfcc_2d=ChannelAttention(256, 8)( x_mfcc_2d)\n","  x_mfcc_2d=SpatialAttention(7)(x_mfcc_2d)\n","\n","\n","\n","  #------------- ASR-----------#\n","\n","  lstm_asr = Bidirectional(LSTM(64,return_sequences=True),name=\"wav2vec_bi\")(x_asr)\n","  lstm_asr=TimeDistributed(Dense(149),name=\"wav2vec_time\")(lstm_asr)\n","\n","  lstm_asr=tf.keras.layers.BatchNormalization()(lstm_asr, training=True)\n","  lstm_asr = Dropout(0.4)(lstm_asr)\n","\n","\n","  lstm_asr=GlobalAveragePooling1D()(lstm_asr)\n","  # #------------- ASR-----------#\n","\n","\n","  #------------- ff-----------#\n","\n","  lstm_ff = Bidirectional(LSTM(64,return_sequences=True),name=\"ff_bi\")(x_ff)\n","  lstm_ff=TimeDistributed(Dense(64),name=\"ff_time\")(lstm_ff)\n","\n","  lstm_ff=tf.keras.layers.BatchNormalization()(lstm_ff, training=True)\n","  lstm_ff = Dropout(0.4)(lstm_ff)\n","\n","  lstm_ff=GlobalAveragePooling1D()(lstm_ff)\n","  # #------------- ff-----------#\n","\n","   #------------- ac-----------#\n","\n","  lstm_ac = Bidirectional(LSTM(64,return_sequences=True),name=\"ac_bi\")(x_ac)\n","  lstm_ac=TimeDistributed(Dense(64),name=\"ac_time\")(lstm_ac)\n","\n","  lstm_ac=tf.keras.layers.BatchNormalization()(lstm_ac, training=True)\n","  lstm_ac = Dropout(0.4)(lstm_ac)\n","\n","  lstm_ac=GlobalAveragePooling1D()(lstm_ac)\n","  # #------------- ac-----------#\n","\n","  lstm_zc = Bidirectional(LSTM(64,return_sequences=True),name=\"zc_bi\")(x_zc)\n","  lstm_zc=TimeDistributed(Dense(64),name=\"zc_time\")(lstm_zc)\n","\n","  lstm_zc=tf.keras.layers.BatchNormalization()(lstm_zc, training=True)\n","  lstm_zc = Dropout(0.4)(lstm_zc)\n","\n","  lstm_zc=GlobalAveragePooling1D()(lstm_zc)\n","  # lstm_zc = Dense(128, activation='relu')(lstm_zc)\n","  # #------------- ac-----------#\n","\n","  #------------- RCNN-----------#\n","  lstm_mfcc_2d = Permute((2, 1, 3))(x_mfcc_2d)\n","  # lstm_mfcc_2d = Permute((2, 1, 3))(x_mfcc_2d)\n","  lstm_mfcc_2d = Reshape((input_mfcc_2d.shape[-2], -1))(lstm_mfcc_2d)\n","  lstm_mfcc_2d = Bidirectional(LSTM(64,return_sequences=True))(lstm_mfcc_2d)\n","  lstm_mfcc_2d=TimeDistributed(Dense(64))(lstm_mfcc_2d)\n","\n","  lstm_mfcc_2d=tf.keras.layers.BatchNormalization()(lstm_mfcc_2d, training=True)\n","  lstm_mfcc_2d = Dropout(0.4)(lstm_mfcc_2d)\n","  # attention_layer = ChannelAttention(47, 64)(lstm_mfcc_2d)\n","\n","  lstm_mfcc_2d=GlobalAveragePooling1D()(lstm_mfcc_2d)\n","  concat = concatenate([lstm_mfcc_2d,lstm_asr,lstm_ff,lstm_ac,lstm_zc], axis=-1, name ='concat')\n","\n","\n","  out = Dense(model_paramerters['no_of_classes'], activation='sigmoid')(concat)\n","  # model = tf.keras.models.Model(inputs = [input_mfcc_2d,input_asr,input_ff], outputs = out)\n","  model = tf.keras.models.Model(inputs = [input_mfcc_2d,input_asr,input_ff,input_ac,input_zc], outputs = out)\n","  # model = tf.keras.models.Model(inputs = [input_mfcc_2d,input_asr,input_ff,input_ac], outputs = out)\n","  return model"],"id":"YcpncgQC38k9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Xjv-PUO4P0N"},"outputs":[],"source":["model=proposed_model_build()\n","plot_model(model, show_shapes=True, to_file='model.png')\n","model.summary()"],"id":"0Xjv-PUO4P0N"},{"cell_type":"code","source":["class WeightedAverage(tf.keras.layers.Layer):\n","\n","    def __init__(self, n_output):\n","        super(WeightedAverage, self).__init__()\n","        self.W = tf.Variable(initial_value=tf.random.uniform(shape=[1,1,n_output], minval=0, maxval=1),\n","            trainable=True) # (1,1,n_inputs)\n","\n","    def call(self, inputs):\n","\n","        # inputs is a list of tensor of shape [(n_batch, n_feat), ..., (n_batch, n_feat)]\n","        # expand last dim of each input passed [(n_batch, n_feat, 1), ..., (n_batch, n_feat, 1)]\n","        inputs = [tf.expand_dims(i, -1) for i in inputs]\n","        inputs = Concatenate(axis=-1)(inputs) # (n_batch, n_feat, n_inputs)\n","        weights = tf.nn.softmax(self.W, axis=-1) # (1,1,n_inputs)\n","        # weights sum up to one on last dim\n","\n","        return tf.reduce_sum(weights*inputs, axis=-1) # (n_batch, n_feat)"],"metadata":{"id":"pt28cRI1jIMk"},"id":"pt28cRI1jIMk","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1669527573466,"user":{"displayName":"Abdulkarim Albanna","userId":"10002314189592944663"},"user_tz":0},"id":"QmMkaNbb4t8U","outputId":"d19f53b5-5bb5-437f-c343-ea1fd25da8e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '*.hdf5': No such file or directory\n"]}],"source":["!rm *.hdf5\n","early_stopping=EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta=1e-4)"],"id":"QmMkaNbb4t8U"},{"cell_type":"code","source":["y_test=test.iloc[:,-5:]\n","y_test['I']"],"metadata":{"id":"BRiSZhQx0SVC"},"id":"BRiSZhQx0SVC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_temp=test\n","train_temp=train"],"metadata":{"id":"mR_IOYTD-8v0"},"id":"mR_IOYTD-8v0","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KWGxCBt4zul"},"outputs":[],"source":["#kfold = KFold(n_splits=10, shuffle=True)\n","results=[]\n","labels=['P','B','S','W','I']\n","\n","for exp in range(0,5):\n","  y_test=test_temp.iloc[:,-5:]\n","  Y_train=train_temp.iloc[:,-5:]\n","  index_test=labels[exp]\n","  index_train=labels[exp]\n","  y_tes=y_test[index_test]\n","  # print(y_tes)\n","  Y_tra=Y_train[index_train]\n","\n","\n","  Y_train=np.array(Y_tra)\n","  y_valid=np.array(y_tes)\n","  Y=Y_train\n","  y_test=y_valid\n","  print(Y)\n","  class_weights=class_weight.compute_class_weight(class_weight='balanced',classes=np.unique(Y),y=Y)\n","  class_weights={0:class_weights[0],1:class_weights[1]}\n","  print(class_weights)\n","\n","\n","\n","  kfold = KFold(n_splits=10, shuffle=True,random_state=1)\n","  i=0\n","  # model =proposed_model_build()\n","  for train, test in kfold.split(X_mfcc_2d, Y):\n","    i=i+1\n","    print(\"Experiment \",exp,\"Fold \",i)\n","    model =proposed_model_build()\n","    opt = tf.optimizers.Adam(learning_rate=0.0001)\n","    # metrics=[custom_f1]\n","    # model.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])\n","    model.compile(optimizer=opt,loss=tfa.losses.SigmoidFocalCrossEntropy(),metrics=['accuracy'])\n","    chk_point=ModelCheckpoint(filepath='/content/drive/MyDrive/classification_model/Thesis_experiments/extra_experiments/optimized_model/results/fb_all/models/mfcc2d-model-Exp-'+str(index_train)+\"-Fold-\"+str(i)+'.hdf5', verbose=1,\n","                            save_best_only=True, save_weights_only=True, mode='auto')\n","    # 256\n","    # Focal loss\n","\n","    # history=model.fit([X_mfcc_2d[train],X_asr[train],X_ff[train],X_ac[train]],Y[train],epochs=300, batch_size=256,verbose=1,validation_data=([X_mfcc_2d[test],X_asr[test],X_ff[test],X_ac[test]],Y[test] ),callbacks=[early_stopping,chk_point])\n","    # history=model.fit(X_mfcc_2d[train],Y[train],epochs=300, batch_size=256,verbose=1,validation_data=([X_mfcc_2d[test]],Y[test] ),callbacks=[early_stopping,chk_point])\n","    # history=model.fit([X_mfcc_2d[train],X_asr[train],X_ff[train]],Y[train],epochs=300, batch_size=256,verbose=1,validation_data=([X_mfcc_2d[test],X_asr[test],X_ff[test] ],Y[test]),callbacks=[early_stopping,chk_point])\n","    history=model.fit([X_mfcc_2d[train],X_asr[train],X_ff[train],X_ac[train],X_zc[train]],Y[train],epochs=300, batch_size=128,verbose=1,validation_data=([X_mfcc_2d[test],X_asr[test],X_ff[test],X_ac[test],X_zc[test]],Y[test] ),callbacks=[early_stopping,chk_point])\n","\n","\n","    t_loss_values = history.history['loss']\n","    epochs = range(1, len(t_loss_values)+1)\n","    loss_values = history.history['val_loss']\n","\n","    plt.plot(epochs, t_loss_values, label='Training Loss')\n","    plt.plot(epochs, loss_values, label='Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.show()\n","    # scores = model.evaluate([X_mfcc_2d[test]], Y[test], verbose=1)\n","    # scores = model.evaluate([X_mfcc_2d[test],X_asr[test],X_ff[test]], Y[test], verbose=1)\n","    scores = model.evaluate([X_mfcc_2d[test],X_asr[test],X_ff[test],X_ac[test],X_zc[test]], Y[test], verbose=1)\n","    results.append(scores[1]*100)\n","    # print(\" Kareem: %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n","    print(\"*\"*100)\n","\n","\n"],"id":"4KWGxCBt4zul"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6toa_zJMjzIh"},"outputs":[],"source":["y_test"],"id":"6toa_zJMjzIh"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnNr3fTe84wS"},"outputs":[],"source":["all_true_list=[]\n","opt = tf.optimizers.Adam(learning_rate=0.0001)\n","target_names = ['P','B','S','W','I']\n","\n","# target_names = ['S','W','I']\n","X_asr=np.array(X_test_asr).reshape(len(X_test_asr),149,32)\n","\n","X_ff=np.array(X_test_ff).reshape(len(X_test_ff),3,301)\n","X_ac=np.array(X_test_ac).reshape(len(X_test_ac),1,300)\n","X_zc=np.array(X_test_zc).reshape(len(X_test_zc),1,301)\n","X_mfcc_2d=np.array(X_test_mfcc_2d).reshape(len(X_test_mfcc_2d),20,301)\n","\n","\n","\n","\n","\n","\n","\n","Y=y_test\n","\n","f1_avg=[[],[],[],[],[]]\n","recall_avg=[[],[],[],[],[]]\n","acc_avg=[[],[],[],[],[]]\n","precision_avg=[[],[],[],[],[]]\n","df_result=pd.DataFrame()\n","\n","from sklearn.metrics import f1_score,recall_score,average_precision_score\n","from tqdm import tqdm\n","for exp in range(0,5):\n","  y_test=test_temp.iloc[:,-5:]\n","  y_tes=y_test[target_names[exp]]\n","  y_test = y_tes.astype(int)\n","\n","  for i in range(1,11):\n","      print(\"Results for : \",target_names[exp])\n","      model.load_weights(filepath='/content/drive/MyDrive/classification_model/Thesis_experiments/extra_experiments/optimized_model/results/fb_all/models/mfcc2d-model-Exp-'+target_names[exp]+\"-Fold-\"+str(i)+'.hdf5')\n","      model.compile(optimizer=opt,loss=tfa.losses.SigmoidFocalCrossEntropy(),metrics=['accuracy'])\n","\n","      # y_scores = model.predict([X_asr])\n","      # y_scores = model.predict([X_mfcc_2d,X_asr,X_ff,X_ac,X_zc])\n","      # y_scores = model.predict([X_mfcc,X_asr,X_ff])\n","      y_scores = model.predict([X_mfcc_2d,X_asr,X_ff,X_ac,X_zc])\n","      y_pred=[]\n","      for sample in  y_scores:\n","        y_pred.append([1 if i>=0.5 else 0 for i in sample ] )\n","      y_pred = np.array(y_pred)\n","\n","\n","      label_names = ['Prolongation', 'Block', 'Sound', 'Word','Interjection','Fluent']\n","      matrix=confusion_matrix(y_test, y_pred)\n","      tp=matrix[0][0]\n","      tn=matrix[1][1]\n","      fp=matrix[1][0]\n","      fn=matrix[0][1]\n","      accuracy=(matrix[0][0]+matrix[1][1])/831\n","\n","      f1_macro=f1_score(y_test, y_pred, average='macro')\n","      f1_weighted=f1_score(y_test, y_pred, average='weighted')\n","      f1_micro=f1_score(y_test, y_pred, average='micro')\n","\n","      recall_macro=recall_score(y_test, y_pred, average='macro')\n","      recall_weighted=recall_score(y_test, y_pred, average='weighted')\n","      recall_micro=recall_score(y_test, y_pred, average='micro')\n","\n","\n","      pre_macro=precision_score(y_test, y_pred,average='macro')\n","      pre_weighted=precision_score(y_test, y_pred, average='weighted')\n","      pre_micro=precision_score(y_test, y_pred, average='micro')\n","\n","\n","\n","      print(accuracy,f1_weighted,recall_weighted,pre_weighted)\n","\n","      print(matrix)\n","      print(classification_report(y_test, y_pred))\n","      s = pd.Series([label_names[exp],tp,tn,fp,fn,accuracy,f1_macro,f1_weighted,f1_micro,recall_macro,recall_weighted,recall_micro,pre_macro,pre_weighted,pre_micro])\n","\n","      df_result= df_result.append(s ,ignore_index=True)\n","\n"],"id":"cnNr3fTe84wS"},{"cell_type":"code","execution_count":null,"metadata":{"id":"irV1zMIA9dKL"},"outputs":[],"source":["header=[label_names[exp],\"tp\",\"tn\",\"fp\",\"fn\",\"accuracy\",\"f1_macro\",\"f1_weighted\",\"f1_micro\",\"recall_macro\",\"recall_weighted\",\"recall_micro\",\"pre_macro\",\"pre_weighted\",\"pre_micro\"]\n","df_result.columns =header\n","df_result"],"id":"irV1zMIA9dKL"},{"cell_type":"code","source":["df_result.to_csv(\"/content/drive/MyDrive/classification_model/Thesis_experiments/extra_experiments/optimized_model/results/csv/fb_all.csv\")"],"metadata":{"id":"1JyEnZd6b14N"},"id":"1JyEnZd6b14N","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1p8xVPa0tGxNU8b9-yj_-B4Zb1RXqGFKh","timestamp":1652864608208},{"file_id":"1DPil2XpMTg-5-SdSf2o7nXbOiTUQryll","timestamp":1652853411498},{"file_id":"1Qn3EkAR8h6k88VB8S8IOmGb5H5zG1bId","timestamp":1652547849780},{"file_id":"13lBc8KPNM8_LS_iIqpLRuLwkpKYGXCon","timestamp":1652544261780},{"file_id":"1yUqfuN_Bu-0E4pa2afim-TVQfIr4pZMQ","timestamp":1652385135050}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":5}